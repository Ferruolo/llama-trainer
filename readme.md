# LLama Trainer

Custom setup for training llama.
Can't afford to use huggingface format as I need pytorch flexibility
for future endevors with these weights.

Trying to make this as flexible/simple as possible for reuse



## TODOS:
- Quantize using double quant from QLoRA
- Setup LORAs
- Make sure that I'm syncing weights right





